{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%run ../Boxes/ORB_box.ipynb\n",
    "%run ../Boxes/HOG_box.ipynb\n",
    "%run ../Boxes/LBP_box.ipynb\n",
    "%run ../Boxes/Hough_transform_box.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_images = []\n",
    "feature_vectors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder_path):\n",
    "  print(\"in function\")\n",
    "  # Set the path to the folder containing the images\n",
    "  # Create an empty list to store the images\n",
    "  feature_vectors = []\n",
    "  # Loop through all the images in the folder\n",
    "  for imagefile in os.listdir(folder_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(os.path.join(folder_path, imagefile))\n",
    "    # Convert the color channels from BGR to RGB\n",
    "    hog_feature_vector = HOG_box(image)\n",
    "    # Append the image to the list of images\n",
    "    feature_vectors.append(hog_feature_vector)\n",
    "    # Append the label to the list of labels\n",
    "    del image\n",
    "\n",
    "  return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_linux():\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    Y, X = load_images_from_folder('/home/ahmedalaa/Study/College/NN/Hand-Gesture-Recognition/resizedData')\n",
    "\n",
    "    return np.array(Y), np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For the windows farmers\n",
    "def load_dataset_win():\n",
    "    X = []\n",
    "    Y = []\n",
    "    x  = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/dataset/men/0')\n",
    "    X += x\n",
    "    Y+=np.repeat(0,len(x)).tolist()\n",
    "    x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/dataset/women/0')\n",
    "    X += x\n",
    "    Y+=np.repeat(0,len(x)).tolist()\n",
    "    x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/dataset/men/1')\n",
    "    X += x\n",
    "    Y+=np.repeat(1,len(x)).tolist()\n",
    "    x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/dataset/women/1')\n",
    "    X += x\n",
    "    Y+=np.repeat(1,len(x)).tolist()\n",
    "    x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/dataset/men/2')\n",
    "    X += x\n",
    "    Y+=np.repeat(2,len(x)).tolist()\n",
    "    x= load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/dataset/women/2')\n",
    "    X += x\n",
    "    Y+=np.repeat(2,len(x)).tolist()\n",
    "    x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/dataset/men/3')\n",
    "    X += x\n",
    "    Y+=np.repeat(3,len(x)).tolist()\n",
    "    x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/dataset/women/3')\n",
    "    X += x\n",
    "    Y+=np.repeat(3,len(x)).tolist()\n",
    "    x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/dataset/men/4')\n",
    "    X += x\n",
    "    Y+=np.repeat(4,len(x)).tolist()\n",
    "    x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/dataset/women/4')\n",
    "    X += x\n",
    "    Y+=np.repeat(4,len(x)).tolist()\n",
    "    x= load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/dataset/men/5')\n",
    "    X += x\n",
    "    Y+=np.repeat(5,len(x)).tolist()\n",
    "    x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/dataset/women/5')\n",
    "    X += x\n",
    "    Y+=np.repeat(5,len(x)).tolist()\n",
    "    return X,Y\n",
    "#     # X_train = []\n",
    "#     # Y_train = []\n",
    "#     # X_test=[]\n",
    "#     # Y_test=[]\n",
    "#     #   #================================================Training===============================================#\n",
    "#     # ##0:\n",
    "#     # x= load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/Distrubute_dataset/men/0')\n",
    "#     # X_train += x\n",
    "#     # Y_train +=np.repeat(0,len(x)).tolist()\n",
    "#     # x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/Distrubute_dataset/women/0')\n",
    "#     # X_train += x\n",
    "#     # Y_train += np.repeat(0,len(x)).tolist()\n",
    "#     # ##1:\n",
    "#     # x= load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/Distrubute_dataset/men/1')\n",
    "#     # X_train += x\n",
    "#     # Y_train += np.repeat(1,len(x)).tolist()\n",
    "#     # x= load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/Distrubute_dataset/women/1')\n",
    "#     # X_train += x\n",
    "#     # Y_train += np.repeat(1,len(x)).tolist()\n",
    "#     # ##2:\n",
    "#     # x= load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/Distrubute_dataset/men/2')\n",
    "#     # X_train += x\n",
    "#     # Y_train += np.repeat(2,len(x)).tolist()\n",
    "#     # x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/Distrubute_dataset/women/2')\n",
    "#     # X_train += x\n",
    "#     # Y_train += np.repeat(2,len(x)).tolist()\n",
    "#     # ##3:\n",
    "#     # x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/Distrubute_dataset/men/3')\n",
    "#     # X_train += x\n",
    "#     # Y_train += np.repeat(3,len(x)).tolist()\n",
    "#     # x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/Distrubute_dataset/women/3')\n",
    "#     # X_train += x\n",
    "#     # Y_train += np.repeat(3,len(x)).tolist()\n",
    "#     # ##4:\n",
    "#     # x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/Distrubute_dataset/men/4')\n",
    "#     # X_train += x\n",
    "#     # Y_train += np.repeat(4,len(x)).tolist()\n",
    "#     # x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/Distrubute_dataset/women/4')\n",
    "#     # X_train += x\n",
    "#     # Y_train += np.repeat(4,len(x)).tolist()\n",
    "#     # ##5:\n",
    "#     # x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/Distrubute_dataset/men/5')\n",
    "#     # X_train += x\n",
    "#     # Y_train += np.repeat(5,len(x)).tolist()\n",
    "#     # x = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/Distrubute_dataset/women/5')\n",
    "#     # X_train += x\n",
    "#     # Y_train += np.repeat(5,len(x)).tolist()\n",
    "#     # #================================================validation===============================================#\n",
    "#     # Y_test = np.repeat(np.arange(6), 16)\n",
    "#     # X_test=load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/Distrubute_dataset/validation')\n",
    "#     # return np.array(X_train), np.array(Y_train), np.array(X_test),np.array(Y_test)\n",
    "# For the windows farmers\n",
    "# def load_all_dataset__win():\n",
    "#   Y,X = load_images_from_folder('E:/2nd term 3rd year/Neural Network/Project/data')\n",
    "#   print(Y)\n",
    "#   return np.array(Y),np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in function\n",
      "in function\n",
      "in function\n",
      "in function\n",
      "in function\n",
      "in function\n",
      "in function\n",
      "in function\n",
      "in function\n",
      "in function\n",
      "in function\n",
      "in function\n"
     ]
    }
   ],
   "source": [
    "X,Y=load_dataset_win()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate the arrays element-wise\n",
    "# # Reshape the second array to be a 2D array of size (numberofsamples, 1)\n",
    "# # print(y)\n",
    "# Y_train = Y_train.reshape((Y_train.shape[0], 1))\n",
    "# # open the file in append mode\n",
    "# with open('Label_FeatureVector.csv', 'w') as f:\n",
    "#     f.truncate(0)\n",
    "\n",
    "# # # open the file in append mode and write new data to it\n",
    "# with open('Label_FeatureVector.csv', 'a') as f:\n",
    "#         # Save in csv file\n",
    "#         np.savetxt('Label_FeatureVector.csv',  np.concatenate((Y_train, X_train), axis=1), delimiter=',')\n",
    "\n",
    "# print(y)\n",
    "# Y = Y.reshape((Y.shape[0], 1))\n",
    "# # open the file in append mode\n",
    "# with open('Label_FeatureVector.csv', 'w') as f:\n",
    "#     f.truncate(0)\n",
    "\n",
    "# # # open the file in append mode and write new data to it\n",
    "# with open('Label_FeatureVector.csv', 'a') as f:\n",
    "#         # Save in csv file\n",
    "#         np.savetxt('Label_FeatureVector.csv',  np.concatenate((Y, X), axis=1), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an SVM classifier\n",
    "# clf = svm.SVC(kernel='rbf', C=10,gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptors=[]\n",
    "# for i in range(len(X)):\n",
    "#     descriptors.append(X[i].flatten())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_arry=np.array(X)\n",
    "Y_arry=np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_arry, Y_arry, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_vaild,Y_train, Y_valid = train_test_split(X_train,Y_train, test_size=0.15,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_flat=np.concatenate(X_train)\n",
    "# # Cluster descriptors using K-means\n",
    "# kmeans = KMeans(n_clusters=100)\n",
    "# kmeans.fit(X_train_flat)\n",
    "# #====================train\n",
    "# train_histograms = []\n",
    "# for des in X_train :\n",
    "#     histogram = np.zeros(kmeans.n_clusters)\n",
    "#     if  des is not None:\n",
    "#         cluster_indices = kmeans.predict(des)\n",
    "#         for index in cluster_indices:\n",
    "#             histogram[index] += 1\n",
    "#     train_histograms.append(histogram)\n",
    "# #=============================Test\n",
    "# test_histograms = []\n",
    "# for des in X_test:\n",
    "#     histogram = np.zeros(kmeans.n_clusters)\n",
    "#     if des is not None:\n",
    "#         cluster_indices = kmeans.predict(des)\n",
    "#         for index in cluster_indices:\n",
    "#             histogram[index] += 1\n",
    "#     test_histograms.append(histogram)\n",
    "# clf = svm.SVC(kernel='rbf', C=10,gamma='scale')\n",
    "# # Train the SVM classifier on the training data\n",
    "# clf.fit(train_histograms, Y_train)\n",
    "# # Make predictions on the testing data\n",
    "# y_pred = clf.predict(test_histograms)\n",
    "# # Calculate the accuracy of the SVM classifier\n",
    "# accuracy = accuracy_score(Y_test, y_pred)\n",
    "# # Print the accuracy of the SVM classifier\n",
    "# print(\"Accuracy:\", accuracy*100, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define parameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [3, 5, 10],\n",
    "#     'min_samples_split': [2, 5, 10]\n",
    "# }\n",
    "# rfc = RandomForestClassifier(random_state=1)\n",
    "# # Perform grid search using cross-validation\n",
    "# grid_search = GridSearchCV(rfc, param_grid=param_grid, cv=5)\n",
    "# grid_search.fit(X_train, Y_train)\n",
    "# # print best parameters and score\n",
    "# print(\"Best parameters: \", grid_search.best_params_)\n",
    "# print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "# # evaluate model on testing dataset\n",
    "# score = grid_search.score(X_test, Y_test)\n",
    "# print(\"Test score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the XGBoost classifier\n",
    "# # gb_clf = GradientBoostingClassifier(random_state=1)\n",
    "# clf= KNeighborsClassifier()\n",
    "# # # Define the parameter grid to search over\n",
    "# param_grid = {'n_neighbors': [1, 3, 5, 7, 9]}\n",
    "\n",
    "# # Perform grid search using cross-validation\n",
    "# grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5)\n",
    "# grid_search.fit(X_train, Y_train)\n",
    "# # print best parameters and score\n",
    "# print(\"Best parameters: \", grid_search.best_params_)\n",
    "# print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "# # evaluate model on testing dataset\n",
    "# score = grid_search.score(X_test, Y_test)\n",
    "# print(\"Test score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define SVM classifier\n",
    "# svc = svm.SVC()\n",
    "# # define parameter grid\n",
    "# param_grid = {'C': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'gamma': ['scale', 'auto']}\n",
    "# # perform grid search\n",
    "# grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
    "# grid_search.fit(X_train,Y_train)\n",
    "\n",
    "# # print best parameters and score\n",
    "# print(\"Best parameters: \", grid_search.best_params_)\n",
    "# print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "# # evaluate model on testing dataset\n",
    "# score = grid_search.score(X_test, Y_test)\n",
    "# print(\"Test score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy hard: 76.64233576642336 %\n",
      "Accuracy soft: 79.1970802919708 %\n",
      "=======================Valid====================\n",
      "Accuracy hard: 78.11158798283262 %\n",
      "Accuracy soft: 78.96995708154506 %\n"
     ]
    }
   ],
   "source": [
    "estimator = []\n",
    "estimator.append(('FR', RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=2, random_state=1)))\n",
    "estimator.append(('SVC', svm.SVC(kernel='rbf', C=10, gamma='scale', probability=True)))\n",
    "estimator.append(('KNN', KNeighborsClassifier(n_neighbors=5)))\n",
    "  \n",
    "# Voting Classifier with hard voting\n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
    "vot_hard.fit(X_train, Y_train)\n",
    "y_pred = vot_hard.predict(X_test)\n",
    "  \n",
    "# using accuracy_score metric to predict accuracy\n",
    "score = accuracy_score(Y_test, y_pred)\n",
    "print(\"Accuracy hard:\", score*100, '%')\n",
    "  \n",
    "# Voting Classifier with soft voting\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "vot_soft.fit(X_train, Y_train)\n",
    "y_pred = vot_soft.predict(X_test)\n",
    "  \n",
    "# using accuracy_score\n",
    "score = accuracy_score(Y_test, y_pred)\n",
    "print(\"Accuracy soft:\", score*100, '%')\n",
    "\n",
    "print(\"=======================Valid====================\")\n",
    "\n",
    "y_pred = vot_hard.predict(X_vaild)\n",
    "# using accuracy_score metric to predict accuracy\n",
    "score = accuracy_score(Y_valid, y_pred)\n",
    "print(\"Accuracy hard:\", score*100, '%')\n",
    "\n",
    "y_pred = vot_soft.predict(X_vaild) \n",
    "# using accuracy_score\n",
    "score = accuracy_score(Y_valid, y_pred)\n",
    "print(\"Accuracy soft:\", score*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.56204379562044 %\n"
     ]
    }
   ],
   "source": [
    "# Create an SVM classifier\n",
    "# clf = svm.SVC(kernel='rbf', C=10,gamma='scale')\n",
    "# Best parameters:  {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
    "# Best parameters:  {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
    "# Best score:  0.7823730879749224\n",
    "# Test score:  0.791970802919708\n",
    "clf = svm.SVC(kernel='rbf', C=10,gamma='scale')\n",
    "# Train the SVM classifier on the training data\n",
    "clf.fit(X_train, Y_train)\n",
    "# Make predictions on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "# Calculate the accuracy of the SVM classifier\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "# Print the accuracy of the SVM classifier\n",
    "print(\"Accuracy:\", accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m clf \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,min_samples_split\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Train the SVM classifier on the training data\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train, Y_train)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Make predictions on the testing data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    477\u001b[0m )(\n\u001b[0;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    479\u001b[0m         t,\n\u001b[0;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    481\u001b[0m         X,\n\u001b[0;32m    482\u001b[0m         y,\n\u001b[0;32m    483\u001b[0m         sample_weight,\n\u001b[0;32m    484\u001b[0m         i,\n\u001b[0;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    489\u001b[0m     )\n\u001b[0;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1044\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1042\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1044\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1045\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1048\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:859\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 859\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    860\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:777\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    776\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 777\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    778\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    779\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;49;00m func, args, kwargs \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 184\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    890\u001b[0m         X,\n\u001b[0;32m    891\u001b[0m         y,\n\u001b[0;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create an Random forest classifier\n",
    "# Best parameters:  {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "# Best parameters:  {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "# Best score:  0.7321597538675876\n",
    "# Test score:  0.7262773722627737\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=10,min_samples_split=2, random_state=1)\n",
    "# Train the SVM classifier on the training data\n",
    "clf.fit(X_train, Y_train)\n",
    "# Make predictions on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "# Calculate the accuracy of the SVM classifier\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "# Print the accuracy of the SVM classifier\n",
    "print(\"Accuracy:\", accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.379562043795616 %\n"
     ]
    }
   ],
   "source": [
    "# Create an SVM classifier\n",
    "# Best parameters:  {'n_neighbors': 7}\n",
    "# Define the XGBoost classifier\n",
    "# clf = GradientBoostingClassifier(n_estimators=50, max_depth=5, learning_rate=0.1, random_state=1)\n",
    "clf = GaussianNB()\n",
    "# Best parameters:  {'n_neighbors': 7}\n",
    "# Best score:  0.7070240617653034\n",
    "# Test score:  0.708029197080292\n",
    "# clf=KNeighborsClassifier(n_neighbors=5)\n",
    "# Train the SVM classifier on the training data\n",
    "clf.fit(X_train, Y_train)\n",
    "# Make predictions on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "# Calculate the accuracy of the SVM classifier\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "# Print the accuracy of the SVM classifier\n",
    "print(\"Accuracy:\", accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y_test.csv', 'w') as f:\n",
    "    f.truncate(0)\n",
    "with open('y_pred.csv', 'w') as f:\n",
    "    f.truncate(0)\n",
    "\n",
    "# open the file in append mode and write new data to it\n",
    "with open('y_test.csv', 'a') as f:\n",
    "        # Save in csv file\n",
    "        np.savetxt('y_test.csv',  Y_test, delimiter=',')\n",
    "with open('y_pred.csv', 'a') as f:\n",
    "        # Save in csv file\n",
    "        np.savetxt('y_pred.csv',  y_pred, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X_train\n",
    "# del Y_train\n",
    "# del X_train\n",
    "# del X_test\n",
    "# del X_train\n",
    "# del Y_test\n",
    "# del y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1314, 4356)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "def density_estimator(X_train):\n",
    "    # Estimate the probability density function using a Gaussian kernel density estimator\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=10).fit(X_train)\n",
    "    return kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 14.23%\n"
     ]
    }
   ],
   "source": [
    "# Train the density estimator on the HOG features of the training data\n",
    "kde = density_estimator(X_train)\n",
    "\n",
    "# Compute the log-probabilities of the HOG features of the test data under the estimated density\n",
    "log_probs_test = kde.score_samples(X_test)\n",
    "\n",
    "# Reshape log_probs_test to have shape (n_samples, 1)\n",
    "log_probs_test = log_probs_test.reshape(-1, 1)\n",
    "\n",
    "# Calculate predicted labels from log probabilities\n",
    "predicted_labels = np.argmax(log_probs_test, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predicted_labels == Y_test) * 100\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the hard voting trained model to a file\n",
    "with open('hard_voting.pkl', 'wb') as file:\n",
    "  joblib.dump(vot_hard, file)\n",
    "\n",
    "# Save the soft voting trained model to a file\n",
    "with open('soft_voting.pkl', 'wb') as file:\n",
    "  joblib.dump(vot_soft, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
